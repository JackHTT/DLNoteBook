一、词嵌入基础
使用one-hot向量表示单词，虽然它们构造起来很容易，但通常并不是一个好选择。
一个主要的原因是，one-hot词向量无法准确表达不同词之间的相似度，如我们常常使用的余弦相似度。

Word2Vec词嵌入工具的提出正是为了解决上面这个问题，它将每个词表示成一个定长的向量，
并通过在语料库上的预训练使得这些向量能较好地表达不同词之间的相似和类比关系，以引入一定的语义信息。
基于两种概率模型的假设，我们可以定义两种 Word2Vec 模型：
1、Skip-Gram 跳字模型：假设背景词由中心词生成，即建模P(wo∣wc)，其中 wc为中心词，wo为任一背景词；
2、CBOW (continuous bag-of-words) 连续词袋模型：假设中心词由背景词生成，即建模 P(wc∣Wo)，其中 Wo为背景词的集合。

在这里我们主要介绍 Skip-Gram 模型的实现，CBOW 实现与其类似，读者可之后自己尝试实现。后续的内容将大致从以下四个部分展开：
1、PTB 数据集
2、Skip-Gram 跳字模型
3、负采样近似
4、训练模型

二、PTB 数据集
简单来说，Word2Vec 能从语料中学到如何将离散的词映射为连续空间中的向量，并保留其语义上的相似关系。
那么为了训练 Word2Vec 模型，我们就需要一个自然语言语料库，模型将从中学习各个单词间的关系，这里我们使用经典的 PTB 语料库进行训练。
PTB (Penn Tree Bank) 是一个常用的小型语料库，它采样自《华尔街日报》的文章，包括训练集、验证集和测试集。我们将在PTB训练集上训练词嵌入模型。

三、二次采样(再次对原词嵌入模型进行采样)
通常来说，在一个背景窗口中，一个词（如“chip”）和较低频词（如“microprocessor”）同时出现比和较高频词（如“the”）同时出现对训练词嵌入模型更有益。
所以，训练词嵌入模型时可以对词进行二次采样。 具体来说，数据集中每个被索引词 wi将有一定概率被丢弃，
      P(wi)=max(1−sqrt(t/f(wi)),0)
其中 f(wi)是数据集中词 wi的个数与总词数之比，常数 t是一个超参数（实验中设为 10−4）。
可见，只有当 f(wi)>t时，我们才有可能在二次采样中丢弃词 wi，并且越高频的词被丢弃的概率越大。

四、提取中心词和背景词

五、Skip-Gram 跳字模型

六、负采样近似
由于 softmax 运算考虑了背景词可能是词典 V中的任一词，对于含几十万或上百万词的较大词典，就可能导致计算的开销过大。
我们将以 skip-gram 模型为例，介绍负采样 (negative sampling) 的实现来尝试解决这个问题。

七、损失函数
应用负采样方法后，我们可利用最大似然估计的对数等价形式将损失函数定义为如下，根据这个损失函数的定义，我们可以直接使用二元交叉熵损失函数进行计算。

Word2Vec通过Embedding层将One-Hot Encoder转化为低维度的连续值（稠密向量），并且其中意思相近的词将被映射到向量空间中相近的位置。

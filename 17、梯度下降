一、梯度下降
一维梯度下降
证明：沿梯度反方向移动自变量可以减小函数值，
当梯度为正时，那么自变量就是朝减小的方向运动，此时梯度是小于0的，所以函数值减小。
当梯度为负时，那么自变量就是朝增大的方向运动，此时梯度是小于0的，所以函数值减小。

学习率：超参数，越大训练越快，越小训练越慢。

局部极小值

多维梯度下降
损失函数对所有变量求偏导，之后通过学习率进行参数更新。

自适应方法
牛顿法

收敛性分析
只考虑在函数为凸函数，且最小值点上二阶导大于0时的收敛速度：

预处理(Heissan阵辅助梯度下降)
x = x - n*diag(Hf)^-1*dx 
其中n表示学习率，dx表示偏导。

梯度下降与线性搜索（共轭梯度法）

随机梯度下降
随机梯度下降参数更新
对于有n个样本的训练数据集，设fi(x)是第i个样本的损失函数，则目标函数为：
      f(x) = 所有fi(x)的平均数
其梯度为：
      df(x) = 所有dfi(x)的平均数 
使用该梯度一次更新的时间复杂度为O(n)
随机梯度下降更新公式为：  x = x - n*dfi(x) dfi(x)表示随机选取的样本的梯度值。

动态学习率
可能分时间段的动态选择学习率，比如常量，随时间变化的指数型，随时间变化的多项式型。

小批量随机梯度下降

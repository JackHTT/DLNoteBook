一、Momentum
目标函数有关自变量的梯度代表了目标函数在自变量当前位置下降最快的方向。因此，梯度下降也叫做最陡下降(steepest descent)。
在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量。然而，如果自变量的迭代方向仅仅取决于自变量当前位置，这可能会带来一些问题。
对于noisy gradient,我们需要谨慎的选取学习率和batch size,来控制梯度方差和收敛的结果。

二、An ill-conditioned Problem(病态问题,病态系统的解对小扰动非常敏感)
条件数cond表示：线性系统Ax=b在多大程度上能会发生变化（也就是它有多病态，注意不是变态）
Hessian matrix的条件数： cond(H) = H最大特征值/H最小特征值
所以条件数越大，那么该系统越病态。

三、Maximum Learning Rate
通过学习率来保证收敛，但是根据凸优化理论，学习率需要满足一定约束，不宜过大。

四、Preconditioning 
在二阶优化中，我们使用Hessian matrix的逆矩阵(或pseudo inverse)来左乘梯度向量 = H^-1 * g 这样的做法称为precondition,
相当于将H映射为一个单位矩阵，拥有分布均匀的Spectrum,也即我们去优化的等价标函数的Hessian matrix为良好的identity matrix。

同一位置上，目标函数在竖直方向比在水平方向的斜率的绝对值更大。
因此，给定学习率，梯度下降迭代自变量时会使自变量在竖直方向比在水平方向移动幅度更大。
那么，我们需要一个较小的学习率从而避免自变量在竖直方向上越过目标函数最优解。
然而，这会造成自变量在水平方向上朝最优解移动变慢。（可以理解为竖直和水平方向相关）

solution to ill-condition
1、Preconditioning gradient vector: applied in Adam, RMSProp, AdaGrad, Adelta, KFC, Natural gradient 
   and other secord-order optimization algorithms.
2、Averaging history gradient: like momentum, which allows larger learning rates to accelerate convergence; 
   applied in Adam, RMSProp, SGD momentum. 
   
Momentum Algorithm
动量法的提出是为了解决梯度下降的上诉问题。设时间步t的自变量为xt，学习率为 ηt。 在时间步 t=0，动量法创建速度变量 m0，并将其元素初始化成 0。
在时间步 t>0，动量法对每次迭代的步骤做如下修改：

动量超参数满足在0~1范围，当其为0时，动量法等价于小批量随机梯度下降。

使用较小的学习率 η=0.4和动量超参数 β=0.5时，动量法在竖直方向上的移动更加平滑，且在水平方向上更快逼近最优解。
下面使用较大的学习率 η=0.6，此时自变量也不再发散。

Exponential Moving Average（指数加权移动平均）

由指数加权移动平均理解动量法，可以使用较大的学习率，从而使自变量向最优解更快移动。

相对于小批量随机梯度下降，动量法需要对每一个自变量维护一个同它一样形状的速度变量，且超参数里多了动量超参数。

AdaGrad
动量法依赖指数加权移动平均使得自变量的更新方向更加一致，从而降低发散的可能。
而AdaGrad算法，它根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题。

当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。
同动量法一样，AdaGrad算法需要对每个自变量维护同它一样形状的状态变量。

RMSProp
为了解决AdaGrad存在的问题，RMSProp对AdaGrad作了修改，保证自变量每个元素的学习率在迭代过程中就不再一直降低（或不变）。

AdaDelta
AdaDelta算法也针对AdaGrad算法在迭代后期可能较难找到有用解的问题做了改进。有意思的是，AdaDelta算法没有学习率这一超参数。
AdaDelta算法需要对每个自变量维护两个状态变量。

Adam
Adam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均

一、词嵌入进阶
虽然 Word2Vec 已经能够成功地将离散的单词转换为连续的词向量，并能一定程度上地保存词与词之间的近似关系，
但 Word2Vec 模型仍不是完美的，它还可以被进一步地改进：
1、子词嵌入（subword embedding）：FastText 以固定大小的 n-gram 形式将单词更细致地表示为了子词的集合，
   而 BPE (byte pair encoding) 算法则能根据语料库的统计信息，自动且动态地生成高频子词的集合；
2、GloVe 全局向量的词嵌入: 通过等价转换 Word2Vec 模型的条件概率公式，我们可以得到一个全局的损失函数表达，并在此基础上进一步优化模型。
实际中，我们常常在大规模的语料上训练这些词嵌入模型，并将预训练得到的词向量应用到下游的自然语言处理任务中。

二、GloVe 全局向量的词嵌入

三、载入预训练的 GloVe 向量
GloVe 官方 提供了多种规格的预训练词向量，语料库分别采用了维基百科、CommonCrawl和推特等，
语料库中词语总数也涵盖了从60亿到8,400亿的不同规模，同时还提供了多种词向量维度供下游模型使用。

四、求近义词和类比词
求近义词
由于词向量空间中的余弦相似性可以衡量词语含义的相似性（为什么？），我们可以通过寻找空间中的 k 近邻，来查询单词的近义词。

求类比词
除了求近义词以外，我们还可以使用预训练词向量求词与词之间的类比关系，例如“man”之于“woman”相当于“son”之于“daughter”。
求类比词问题可以定义为：对于类比关系中的4个词“a之于 b相当于 c之于 d”，给定前3个词 a,b,c求 d。
求类比词的思路是，搜索与 vec(c)+vec(b)−vec(a)的结果向量最相似的词向量，其中 vec(w)为 w的词向量。

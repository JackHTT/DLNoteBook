一、微调
因为数据量有限，最终训练得到的模型的精度也可能达不到实用的要求。
一个显而易见的解决办法是收集更多的数据；另外一种解决办法是应用迁移学习（transfer learning），将从源数据集学到的知识迁移到目标数据集上。

迁移学习中的一种常用技术：微调（fine tuning），它由4步构成：
1、在源数据集（如ImageNet数据集）上预训练一个神经网络模型，即源模型。
2、创建一个新的神经网络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设计及其参数。
   我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适用于目标数据集。
   我们还假设源模型的输出层跟源数据集的标签紧密相关，因此在目标模型中不予采用。
3、为目标模型添加一个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。
4、在目标数据集（如椅子数据集）上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。

当目标数据集远小于源数据集时，微调有助于提升模型的泛化能力。

当把源模型的输出层的单元数改成目标测试集的类别数，而其它层的结构不变，进行训练时，输出层需要设置较大学习率，其余层由于参数已经比较好了，所以只需微调。

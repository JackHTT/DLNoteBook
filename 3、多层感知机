多层感知机

一、基本知识

多层感知机（MLP）即多层全连接神经网络
全连接的概念表示输入层、输出层以及隐藏层之间的每个单元与上一层的所有单元都有连接

它相比单层神经网络多了隐藏层，隐藏层的单元被称作隐藏单元。

当每个单元的连接都是线性关系的时候，此时神经网络引入了隐藏层，但是依然等价于一个单层神经网络。

二、激活函数

上述问题的根源在于全连接层只是对数据做仿射变换，而多个仿射变换的叠加任然是一个仿射变换。为此需要引入非线性变换，这个非线性函数被称为激活函数。

常用激活函数有： 

Relu函数
ReLU函数提供了一个很简单的非线性变换，定义为： ReLU(x) = max(x,0)  
可以看出只保留正数元素，将负数元素清零

Sigmoid函数
可以将元素的值变换到0和1之间： sigmoid(x) = 1/(1+exp(-x))
sigmoid函数的导数   d(sigmoid(x))/dt = sigmoid(x)*(1-sigmoid(x)) 当x=0时，导数达到最大值，当x越偏离0时，sigmoid函数的导数越接近0.

tanh函数
tanh(双曲正切)函数可以将元素的值变换到-1和1之间： tanh(x)=(1-exp(-2x))/(1+exp(-2x))
当输入接近0时，tanh函数接近线性变换，而且tanh函数在坐标系的原点上对称。
当输入为0时，tanh函数的导数达到最大值1；当输入值越偏离0时，tanh函数的导数越接近0

三、关于激活函数的选择
ReLU函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。
用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题（当导数很小时可以看做导数消失），有时要避免使用sigmoid函数和tanh函数。
在神经网络层数较多的时候，最好使用ReLU函数，ReLU函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。
在选择激活函数的时候可以先选用ReLU函数如果效果不佳可以尝试其它激活函数。

四、多层感知机
多层感知机就是含有一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。多层感知机的层数和各隐藏层中隐藏单元个数都是超参数。
超参数：在开始学习过程之前进行设置值的参数。

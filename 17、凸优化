一、优化与深度学习
优化与估计
尽管优化方法可以最小化深度学习中的损失函数值，但本质上优化方法达到的目标与深度学习的目标并不相同。
优化方法目标：训练集损失函数值
深度学习目标：测试集损失函数值（泛化性）

二、优化在深度学习中的挑战
对于函数梯度为0的点，可通过Hessian matrix来判断是极大值、极小值或鞍点。
当海森矩阵在梯度为0的位置上的特征值全为正时，该函数得到局部最小值。
当海森矩阵在梯度为0的位置上的特征值全为负时，该函数得到局部最大值。
当海森矩阵在梯度为0的位置上的特征值有正有负时，该函数得到鞍点。

1、局部最小值（训练过程找到的很容易使局部最小值）
2、鞍点（saddle point）
总体定义为：曲线沿着某一方向是稳定的，另一条方向是不稳定的奇点，叫做鞍点。
一个不是局部最小值的驻点（一阶导数为0的点）称为鞍点。
判断鞍点的一个充分条件是：函数在一阶导数为零处（驻点）的海森矩阵为不定矩阵。
3、梯度消失

三、凸性（Convexity）
基础
凸集：一个集合是凸的，当且仅当集合中的任意两点之间的线段都在集合中。
凹曲线和凸曲线的分界点称为曲线的拐点。

Jensen不等式
Jensen不等式是关于凸函数性质的不等式，它和凸函数的定义是息息相关的。
凸函数任意两点的割线位于函数图形上方， 这也是Jensen不等式的两点形式。
Jensen不等式是凸函数性质的扩充

性质：1、无局部最小值 2、与凸集的关系 
对于凸函数f(x)，定义集合Sb:={x|x属于X and f(x)≤b},则集合Sb为凸集。
3、凸函数与二阶导数   二阶导数都大于等于0那么函数是凸函数。

限制条件
拉格朗日乘子法
惩罚项
投影

一、注意力机制
在“编码器-解码器(seq2seq)”一节里，解码器在各个时间步依赖相同的背景变量（context vector）来获取输⼊序列信息。
当编码器为循环神经⽹络时，背景变量来⾃它最终时间步的隐藏状态。将源序列输入信息以循环单位状态编码，然后将其传递给解码器以生成目标序列。
然而这种结构存在着问题，尤其是RNN机制实际中存在长程梯度消失的问题，对于较长的句子，我们很难寄希望于将输入的序列转化为定长的向量而保存所有的有效信息，
所以随着所需翻译句子的长度的增加，这种结构的效果会显著下降。
与此同时，解码的目标词语可能只与原输入的部分词语有关，而并不是与所有的输入有关。
例如，当把“Hello world”翻译成“Bonjour le monde”时，“Hello”映射成“Bonjour”，“world”映射成“monde”。
在seq2seq模型中，解码器只能隐式地从编码器的最终状态中选择相应的信息。然而，注意力机制可以将这种选择过程显式地建模。

二、注意力机制框架
Attention是一种常用的带权池化方法，输入由两部分构成：询问（query）和键值对(key-value pairs)。
attention layer得到输出与value的维度一致，对于一个query来说，attention layer会与每一个key计算注意力分数并进行权重的归一化。
输出的向量则是value的加权求和，而每个key计算的权重与value一一对应。

不同的attetion layer的区别在于score函数的选择，

在本节的其余部分，我们将讨论两个常用的注意层 Dot-product Attention 和 Multilayer Perceptron Attention；
随后我们将实现一个引入attention的seq2seq模型并在英法翻译语料上进行训练与测试。

Softmax屏蔽
在深入研究实现之前，我们首先介绍softmax操作符的一个屏蔽操作。

超出2维矩阵的乘法
X和Y是维度分别为(b,n,m)和b(b,m,k)的张量，进行b次二维矩阵乘法后得到Z，维度为(b,n,k)
        Z[i,:,:] = dot(X[i,:,:],Y[i,:,:])  for i = 1,...,n.
        
Dot-product Attention（点积注意力）
The dot product假设query和keys有相同的维度，通过计算query和key转置的乘积来计算attention score，
通常还会除去sqrt(d)减少计算出来的score对维度d的依赖性。转化成矩阵的形式如下:
      f(Q,K) = QK'/sqrt(d)  d表示query的维度
同时这个层支持一批查询和键值对。此外，它支持作为正则化随机删除一些注意力权重。

多层感知机注意力
在多层感知器中，我们首先将query and keys投影，score函数定义如下:
    f(k,q) = v'*tanh(Wk*k+Wq*q)
将key和value在特征的维度上合并(concatenate),然后送至a single hidden layer perceptron这层中hidden layer为h 
and 输出的size为1，隐层激活函数为tanh,无偏置。

总结
1、注意力层显式的选择相关的信息。
2、注意层的内存由键值对组成，因此它的输出接近于键类似于查询的值。

引入注意力机制的Seq2seq模型
本节中将注意机制添加到sequence to sequence 模型中，以显式地使用权重聚合states。
下图展示encoding 和decoding的模型结构，在时间步为t的时候。此刻attention layer保存着encodering看到的所有信息——即encoding的每一步输出。
在decoding阶段，解码器的时刻的隐藏状态被当作query，encoder的每个时间步的hidden states作为key和value进行attention聚合.
Attetion model的输出当作成上下文信息context vector（背景向量），并与解码器输入拼接起来一起送到解码器：

也就是说在编码阶段得到attention layer的key和value，解码阶段得到attention layer的query之后得到输出output作为解码阶段的背景向量(context vector)

解码器
由于带有注意机制的seq2seq的编码器与之前章节中的Seq2SeqEncoder相同，所以在此处我们只关注解码器。
我们添加了一个MLP注意层(MLPAttention)，它的隐藏大小与解码器中的LSTM层相同。然后我们通过从编码器传递三个参数来初始化解码器的状态:
the encoder outputs of all timesteps：encoder输出的各个状态，被用于attetion layer的memory部分，有相同的key和values
the hidden state of the encoder’s final timestep：编码器最后一个时间步的隐藏状态，被用于初始化decoder 的hidden state
the encoder valid length: 
编码器的有效长度，借此，注意层不会考虑编码器输出中的填充标记（Paddings）
在解码的每个时间步，我们使用解码器的最后一个RNN层的输出作为注意层的query。
然后，将注意力模型的输出与输入嵌入向量连接起来，输入到RNN层。虽然RNN层隐藏状态也包含来自解码器的历史信息，
但是attention model的输出显式地选择了enc_valid_len以内的编码器输出，这样attention机制就会尽可能排除其他不相关的信息。

背景向量作为解码器的输入。

seq2seq模型的预测需要人为设定终止条件，设定最长序列长度或者输出[EOS]结束符号，若不加以限制则可能生成无穷长度序列。
注意力机制本身有高效的并行性，但引入注意力并不能改变seq2seq内部RNN的迭代机制，因此无法加速。

一、Transformer
针对之前介绍的CNNs和RNNs
CNNs 易于并行化，却不适合捕捉变长序列内的依赖关系。
RNNs 适合捕捉长距离变长序列的依赖，但是却难以实现并行化处理序列。

使用注意力机制设计了Transformer模型用来结合CNN和RNN的优势，该模型利用attention机制实现了并行化捕捉序列依赖，并且同时处理序列的每个位置的tokens，
上述优势使得Transformer模型在性能优异的同时大大减少了训练时间。

相比于seq2seq模型，Transformer同样基于编码器-解码器架构，其区别主要在于以下三点：（多头注意力层）
Transformer blocks：将seq2seq模型重的循环网络替换为了Transformer Blocks，
该模块包含一个多头注意力层（Multi-head Attention Layers）以及两个position-wise feed-forward networks（FFN）。
对于解码器来说，另一个多头注意力层被用于接受编码器的隐藏状态。
Add and norm：多头注意力层和前馈网络的输出被送到两个“add and norm”层进行处理，该层包含残差结构以及层归一化。
Position encoding：由于自注意力层并没有区分元素的顺序，所以一个位置编码层被用于向序列元素里添加位置信息。

多头注意力层
自注意力(self-attention)模型是一个正规的注意力模型，序列的每一个元素对应的key，value，query是完全一致的。
如图10.3.2 自注意力输出了一个与输入长度相同的表征序列，与循环神经网络相比，自注意力对每个元素输出的计算是并行的，所以我们可以高效的实现这个模块。

多头注意力层包含h个并行的自注意力层，每一个这种层被成为一个head。
对每个头来说，在进行注意力计算之前，我们会将query、key和value用三个现行层进行映射，
这h个注意力头的输出将会被拼接之后输入最后一个线性层进行整合。

多头注意力层里的attention可以是任意的attention function，比如前一节介绍的dot-product attention以及MLP attention。
之后我们将所有head对应的输出拼接起来，送入最后一个线性层（线性回归）进行整合，除此之外，因为多头注意力层保持输入与输出张量的维度不变。

二、基于位置的前馈网络
Transformer 模块另一个非常重要的部分就是基于位置的前馈网络（FFN），它接受一个形状为（batch_size，seq_length, feature_size）的三维张量。
Position-wise FFN由两个全连接层组成，他们作用在最后一维上。
因为序列的每个位置的状态都会被单独地更新，所以我们称他为position-wise，这等效于一个1x1的卷积。

三、Add and Norm
除了上面两个模块之外，Transformer还有一个重要的相加归一化层，它可以平滑地整合输入和其他层的输出，
因此我们在每个多头注意力层和FFN层后面都添加一个含残差连接的Layer Norm层。
这里 Layer Norm 与7.5小节的Batch Norm很相似，唯一的区别在于Batch Norm是对于batch size这个维度进行计算均值和方差的，
而Layer Norm则是对最后一维进行计算。层归一化可以防止层内的数值变化过大，从而有利于加快训练速度并且提高泛化性能。

四、位置编码
与循环神经网络不同，无论是多头注意力网络还是前馈神经网络都是独立地对每个位置的元素进行更新，
这种特性帮助我们实现了高效的并行，却丢失了重要的序列顺序的信息。
为了更好的捕捉序列信息，Transformer模型引入了位置编码去保持输入序列元素的位置。

输出的向量等于输入序列和位置编码相加。

五、编码器
编码器包含一个多头注意力层，一个position-wise FFN，和两个 Add and Norm层。对于attention模型以及FFN模型，
我们的输出维度都是与embedding维度一致的，这也是由于残差连接天生的特性导致的，因为我们要将前一层的输出与原始输入相加并归一化。

整个编码器由n个刚刚定义的Encoder Block堆叠而成，因为残差连接的缘故，中间状态的维度始终与嵌入向量的维度d一致；
同时注意到我们把嵌入向量乘以 sqrt(d)以防止其值过小。

六、解码器
Transformer 模型的解码器与编码器结构类似，然而，除了之前介绍的几个模块之外，编码器部分有另一个子模块。
该模块也是多头注意力层，接受编码器的输出作为key和value，decoder的状态作为query。
与编码器部分相类似，解码器同样是使用了add and norm机制，用残差和层归一化将各个子层的输出相连。

仔细来讲，在第t个时间步，当前输入query，那么self attention接受了第t步以及前t-1步的所有输入x1,…,xt−1
在训练时，由于第t位置的输入可以观测到全部的序列，这与预测阶段的情形项矛盾，
所以我们要通过将第t个时间步所对应的可观测长度设置为t，以消除不需要看到的未来的信息。

对于Transformer解码器来说，构造方式与编码器一样，除了最后一层添加一个dense layer以获得输出的置信度分数。
下面让我们来实现一下Transformer Decoder，除了常规的超参数例如vocab_size embedding_size 之外，
解码器还需要编码器的输出 enc_outputs 和句子有效长度 enc_valid_length。

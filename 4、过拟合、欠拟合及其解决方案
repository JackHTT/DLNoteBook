一、训练误差和泛化误差
训练误差：模型在训练数据集上表现出的误差
泛化误差：模型在任意一个测试数据上表现出的误差的期望，常常通过测试数据集上的误差来近似
计算误差可使用平方损失函数或交叉熵损失函数
机器学习模型应该关注降低泛化误差

二、模型选择
验证数据集：从训练数据集中抽出用来进行模型选择和超参数调优。
K折交叉验证：当训练数据不够用时，预留大量的验证数据显得太奢侈。所以K折交叉验证是改善方法。
使用所有训练数据进行多次训练来达到降低过拟合的效果。

三、过拟合和欠拟合
欠拟合：模型无法得到较低的训练误差
过拟合：模型的训练误差远小于它在测试数据集上的误差。

模型复杂度
描述模型的复杂程度，往往模型越复杂越容易过拟合。

训练数据集大小
如果训练数据集中样本数过少，特别是比模型参数数量更少时，过拟合更容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增加。
在计算资源允许范围内，我们通常希望训练数据集大一些。

解决办法：
1、权重衰减
等价于L2范数正则化。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，从而降低模型的复杂度。

L2范数正则化（在损失函数上加上L2正则化惩罚项）
在原损失函数基础上添加L2范数惩罚项，L2惩罚项指 模型权重参数每个元素的平方和与一个正的常数（超参数惩罚系数）的乘积。
当惩罚系数较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素越接近0.
L2范数正则化又叫权重衰减，权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。

丢弃法
当对隐藏层使用丢弃法时，该层的隐藏单元将有一定的概率p(用来降低过拟合的超参数)被丢弃掉。
丢弃法不改变其输入的期望值，只是对隐藏层的单元进行随机的丢弃,从而降低网络的复杂程度，反向传播时被丢弃的单元的权重的梯度为0.
但是在测试模型的时候，为了拿到更加确定性的结果，一般不使用丢弃法。
